---
title: "Using some popular ML algorithms to predict churning!"
author: "NickD - email: nickydyakov@gmail.com"
date: "2022-08-01"
output: html_document
---

# Loading the needed packages
```{r message = FALSE, warning = FALSE}
library(tidyverse)
library(tidymodels)
library(themis)
library(vip)
library(xgboost)
library(rpart.plot)
```

```{r setup, include = FALSE}
knitr::opts_chunk$set(fig.width = 10, fig.asp = 0.618, out.width = "100%")
```

# Loading the datset
```{r message = FALSE, warning = FALSE}
churn<-read_csv("churn.csv")
df<-churn %>% slice_head(n = 1000)
new_data<-churn %>% slice_tail(n = 10) %>% select(!churn)
```

```{r include = FALSE}
set.seed(2022)
df_split <- initial_split(df, strata = churn)
df_train <- training(df_split)
df_test <- testing(df_split)
```

```{r include = FALSE}
set.seed(2022)
folds <- vfold_cv(df_train, strata = churn)
```

```{r include = FALSE}
clean_rec <- recipe(churn ~., data = df_train) %>%
  step_normalize(all_numeric_predictors()) %>%
  step_dummy(all_nominal_predictors()) %>%
  step_smote(churn)
```

```{r include = FALSE}
model_control <- control_grid(save_pred = TRUE)
model_metrics <- metric_set(roc_auc, sens, spec, accuracy)
```

# 1.XGBoost model
```{r include = FALSE}
xgb_spec <- 
  boost_tree(mtry = tune(),
             trees = tune(),
             tree_depth = tune(),
             learn_rate = tune()) %>%
  set_engine("xgboost") %>% 
  set_mode("classification")
```

```{r include = FALSE}
xgb_wf <-
  workflow() %>%
  add_recipe(clean_rec) %>% 
  add_model(xgb_spec) 
```

```{r include = FALSE}
xgb_grid <- grid_latin_hypercube(
  trees(),
  tree_depth(),
  learn_rate(),
  finalize(mtry(), df_train),
  size = 15)
```

```{r include = FALSE}
doParallel::registerDoParallel()
set.seed(2022)
xgb_tune <- xgb_wf %>%
  tune_grid(folds,
            metrics = model_metrics,
            control = model_control,
            grid = xgb_grid)
```

```{r include = FALSE}
xgb_best <- xgb_tune %>% select_best(metric = "accuracy")
autoplot(xgb_tune)
xgb_best
```

```{r include = FALSE}
xgb_train_results <- xgb_tune %>%
  filter_parameters(parameters = xgb_best) %>%
  collect_metrics()
xgb_train_results
```

```{r include = FALSE}
xgb_test_results <- xgb_wf %>% 
  finalize_workflow(xgb_best) %>%
  last_fit(split = df_split, metrics = model_metrics)
xgb_results <- xgb_test_results %>% 
  collect_metrics()
xgb_results
```

```{r include = FALSE}
xgb_fit <- xgb_wf %>%
  finalize_workflow(xgb_best) %>%
  fit(df_test)
xgb_fit
```

## Predicting on new data
```{r echo = FALSE}
pred_xgb <- augment(xgb_fit, new_data = new_data, type = "prob") %>% 
	select(.pred_class, .pred_churn, .pred_no_churn)
pred_xgb
```

# 2.Random Forest model
```{r include = FALSE}
rf_spec <- 
  rand_forest(mtry = tune(),
              min_n = tune(),
              trees = tune()) %>%
  set_engine("ranger") %>% 
  set_mode("classification")
```

```{r include = FALSE}
rf_wflow <-
  workflow() %>%
  add_recipe(clean_rec) %>% 
  add_model(rf_spec)
```

```{r include = FALSE}
rf_grid <- grid_latin_hypercube(
  finalize(mtry(), df_train),
  trees(),
  min_n(),
  size = 15)
```

```{r include = FALSE}
doParallel::registerDoParallel()
set.seed(2022)
rf_res <-
  rf_wflow %>% 
  tune_grid(
    resamples = folds, 
    metrics = model_metrics,
    control = model_control,
    grid = rf_grid)
```

```{r include = FALSE}
rf_best <- rf_res %>% select_best(metric = "accuracy")
autoplot(rf_res)
rf_best
```

```{r include = FALSE}
rf_train_results <- rf_res %>% 
  filter_parameters(parameters = rf_best) %>% 
  collect_metrics()
rf_train_results
```

```{r include = FALSE}
rf_test_res <- rf_wflow %>% 
  finalize_workflow(rf_best) %>%
  last_fit(split = df_split, metrics = model_metrics)
rf_results <- rf_test_res %>% collect_metrics()
rf_results
```

```{r include = FALSE}
rf_fit <- rf_wflow %>%
  finalize_workflow(rf_best) %>%
  fit(df_test)
rf_fit
```

## Predicting on new data
```{r echo = FALSE}
pred_rf <- augment(rf_fit, new_data = new_data, type = "prob") %>% 
	select(.pred_class, .pred_churn, .pred_no_churn)
pred_rf 
```

# 3.Decision Trees model
```{r include = FALSE}
dt_spec <- 
  decision_tree(
    mode = "classification",
    engine = "rpart",
    cost_complexity = tune(),
    tree_depth = tune(),
    min_n = tune())
```

```{r include = FALSE}
dt_wf <-
  workflow() %>%
  add_recipe(clean_rec) %>% 
  add_model(dt_spec)
```

```{r include = FALSE}
dt_grid <- grid_latin_hypercube(
  cost_complexity(),
  tree_depth(),
  min_n(),
  size = 15)
```

```{r include = FALSE}
doParallel::registerDoParallel()
set.seed(1234)
dt_tune <- dt_wf %>%
  tune_grid(folds,
            metrics = model_metrics,
            control = model_control,
            grid = dt_grid)
```

```{r include = FALSE}
dt_best <- dt_tune %>% select_best(metric = "accuracy")
autoplot(dt_tune)
dt_best
```

```{r include = FALSE}
dt_train_results <- dt_tune %>%
  filter_parameters(parameters = dt_best) %>%
  collect_metrics()
dt_train_results
```

```{r include = FALSE}
dt_test_results <- dt_wf %>% 
  finalize_workflow(dt_best) %>%
  last_fit(split = df_split, metrics = model_metrics)
dt_results <- dt_test_results %>% collect_metrics()
dt_results
```

```{r include = FALSE}
dt_fit <- dt_wf %>%
  finalize_workflow(dt_best) %>%
  fit(df_test)
dt_fit
```

## Predicting on new data
```{r echo = FALSE}
pred_dt <- augment(dt_fit, new_data = new_data, type = "prob") %>% 
	select(.pred_class, .pred_churn, .pred_no_churn)
pred_dt
```

```{r include = FALSE}
train_results <- bind_rows("XGboost" = xgb_train_results,
                           "Random Forest" = rf_train_results,
                           "Decision Trees" = dt_train_results,
                           .id = "model")
```

# 4.Comparing results
```{r fig 01, message = FALSE, warning = FALSE, echo = FALSE}
train_results %>%
  mutate(model = fct_reorder(model, mean)) %>%
  ggplot(aes(.metric, mean, fill = model)) +
  geom_col(position = "dodge") + 
  geom_text(
    aes(x = .metric, y = mean , label = round(mean, 2), group = model),
    position = position_dodge(width = 1), vjust = -0.6, size = 3.5) +
  scale_fill_brewer(palette = "Dark2") +
  labs(y = "Value (mean)", x = "Metric", title =  "Results on training set", fill = "Model:") +
	theme(text = element_text(size = 16))
```

```{r include = FALSE}
test_results <- bind_rows("XGboost" = xgb_results,
                          "Random Forest" = rf_results,
                          "Decision Trees" = dt_results,
                           .id = "model")
```

```{r fig 02, message = FALSE, warning = FALSE, echo = FALSE}
test_results %>%
  mutate(model = fct_reorder(model, .estimate)) %>%
  ggplot(aes(.metric, .estimate, fill = model)) +
  geom_col(position = "dodge") + 
  geom_text(
    aes(x = .metric, y = .estimate , label = round(.estimate, 2), group = model),
    position = position_dodge(width = 1), vjust = -0.6, size = 3.5) +
  scale_fill_brewer(palette = "Dark2") +
  labs(y = "Value", x = "Metric", title =  "Results on test set", fill = "Model:") +
	theme(text = element_text(size = 16))
```

# 5.Confusion matrices
```{r fig 03, message = FALSE, warning = FALSE, echo = FALSE}
collect_predictions(xgb_test_results) %>%
  conf_mat(churn, .pred_class) %>%
  pluck(1) %>%
  as_tibble() %>%
  ggplot(aes(Prediction, Truth, alpha = n)) +
  geom_tile(show.legend = FALSE) +
  geom_text(aes(label = n), colour = "white", alpha = 1, size = 8) +
  labs(y = "Actual result", x = "Predicted result", fill = NULL, title = "Confusion Matrix - XGboost") +
	theme(text = element_text(size = 16))
```

```{r fig 04, message = FALSE, warning = FALSE, echo = FALSE}
collect_predictions(rf_test_res) %>%
  conf_mat(churn, .pred_class) %>%
  pluck(1) %>%
  as_tibble() %>%
  ggplot(aes(Prediction, Truth, alpha = n)) +
  geom_tile(show.legend = FALSE) +
  geom_text(aes(label = n), colour = "white", alpha = 1, size = 8) +
  labs(y = "Actual result", x = "Predicted result", fill = NULL, title = "Confusion Matrix - Random Forest") +
	theme(text = element_text(size = 16))
```

```{r fig 05, message = FALSE, warning = FALSE, echo = FALSE}
collect_predictions(dt_test_results) %>%
  conf_mat(churn, .pred_class) %>%
  pluck(1) %>%
  as_tibble() %>%
  ggplot(aes(Prediction, Truth, alpha = n)) +
  geom_tile(show.legend = FALSE) +
  geom_text(aes(label = n), colour = "white", alpha = 1, size = 8) +
  labs(y = "Actual result", x = "Predicted result", fill = NULL, title = "Confusion Matrix - Decision Trees") +
	theme(text = element_text(size = 16))
```

# 6.ROC curves
```{r fig 06, message = FALSE, warning = FALSE, echo = FALSE}
xgb_test_results %>%
  collect_predictions() %>%
  roc_curve(churn, .pred_churn) %>%
  ggplot(aes(x = 1 - specificity, y = sensitivity)) +
  geom_line(size = 0.5, color = "midnightblue") +
  geom_abline(lty = 2, alpha = 0.5, color = "gray50", size = 0.5) + 
  labs(title = "XGboost - ROC curve") +
	theme(text = element_text(size = 16))
```

```{r fig 07, message = FALSE, warning = FALSE, echo = FALSE}
rf_test_res %>%
  collect_predictions() %>%
  roc_curve(churn, .pred_churn) %>%
  ggplot(aes(x = 1 - specificity, y = sensitivity)) +
  geom_line(size = 0.5, color = "midnightblue") +
  geom_abline(lty = 2, alpha = 0.5, color = "gray50", size = 0.5) + 
  labs(title = "Random Forest - ROC curve") +
	theme(text = element_text(size = 16))
```

```{r fig 08, message = FALSE, warning = FALSE, echo = FALSE}
dt_test_results %>%
  collect_predictions() %>%
  roc_curve(churn, .pred_churn) %>%
  ggplot(aes(x = 1 - specificity, y = sensitivity)) +
  geom_line(size = 0.5, color = "midnightblue") +
  geom_abline(lty = 2, alpha = 0.5, color = "gray50", size = 0.5) + 
  labs(title = "Decision Trees - ROC curve") +
	theme(text = element_text(size = 16))
```

# 7.Variable Importance
```{r fig 09, message = FALSE, warning = FALSE, echo = FALSE}
xgb_test_results %>% 
  pluck(".workflow", 1) %>%
  extract_fit_parsnip() %>% 
  vip(geom = "col", num_features = 10, horiz = TRUE, aesthetics = list(size = 4)) +
  labs(title = "Variable Importance - XGboost") +
	theme(text = element_text(size = 16))
```

```{r fig 10, message = FALSE, warning = FALSE, echo = FALSE}
dt_test_results %>% 
  pluck(".workflow", 1) %>%
  extract_fit_parsnip() %>% 
  vip(geom = "col", num_features = 10, horiz = TRUE, aesthetics = list(size = 4)) +
  labs(title = "Variable Importance - Decision Trees") +
	theme(text = element_text(size = 16))
```

# 8.Plotting the Decision Tree
```{r fig 11, message = FALSE, warning = FALSE, echo = FALSE}
dt_test_results %>%
  extract_fit_engine() %>%
  rpart.plot(roundint = FALSE, cex = 0.6)
```