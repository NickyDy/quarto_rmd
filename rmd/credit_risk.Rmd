---
title: "Credit risk analysis using ML models!"
author: "NickD - email: nickydyakov@gmail.com"
output: html_document
date: "2022-11-09"
---

# Loading the needed packages
```{r message = FALSE, warning = FALSE}
library(tidyverse)
library(tidymodels)
library(janitor)
library(themis)
library(vip)
library(rpart.plot)
```

```{r setup, include = FALSE}
knitr::opts_chunk$set(fig.width = 10, fig.asp = 0.618, message = F, warning = F, echo = F)
```

# Loading the datset
```{r echo=TRUE}
risk <- read_csv("~/Desktop/R/csv/german_credit.csv") %>% clean_names()
df <- risk %>% slice_head(n = 1000)
new_data <- risk %>% slice_tail(n = 10) %>% select(!creditability)
```

# EDA
## Plotting the nominal (target) variable
```{r}
risk %>% 
	count(creditability) %>% 
	ggplot(aes(n, creditability)) +
	geom_col(fill = c("#F8766D", "#00BFC4")) +
	labs(y = "Creditability") +
	theme(text = element_text(size = 16))
```

## Plotting the numerical (predictor) variables
```{r}
risk %>% 
pivot_longer(2:21, names_to = "name", values_to = "value") %>% 
	ggplot(aes(value)) +
	geom_histogram() +
	scale_x_log10() +
	labs(x = "Value (log scale)") +
	theme(text = element_text(size = 12)) +
	facet_wrap(~ name)
```

# MODELLING
```{r include = FALSE}
set.seed(2022)
df_split <- initial_split(df, strata = creditability)
df_train <- training(df_split)
df_test <- testing(df_split)
```

```{r include = FALSE}
set.seed(2022)
folds <- vfold_cv(df_train, strata = creditability)
```

```{r include = FALSE}
clean_rec <- recipe(creditability ~., data = df_train) %>%
	step_corr(all_numeric_predictors()) %>% 
  step_normalize(all_numeric_predictors()) %>%
	step_smote(creditability)
```

```{r include = FALSE}
model_control <- control_grid(save_pred = TRUE)
model_metrics <- metric_set(roc_auc, sens, spec, accuracy)
```

# 1.Logistic Regression model
```{r include = FALSE}
log_spec <- 
	logistic_reg(mixture = 1, penalty = tune()) %>% 
	set_engine("glmnet") %>% 
	set_mode("classification")
```

```{r include = FALSE}
log_wf <- workflow() %>%
	add_recipe(clean_rec) %>%
	add_model(log_spec)
```

```{r include = FALSE}
log_grid <- grid_latin_hypercube(
  penalty(),
  size = 15)
```

```{r include = FALSE}
doParallel::registerDoParallel()
set.seed(2022)
log_tune <- log_wf %>%
  tune_grid(folds,
            metrics = model_metrics,
            control = model_control,
            grid = log_grid)
```

```{r include = FALSE}
log_best <- log_tune %>% select_best(metric = "accuracy")
autoplot(log_tune)
log_best
```

```{r include = FALSE}
log_train_results <- log_tune %>%
  filter_parameters(parameters = log_best) %>%
  collect_metrics()
log_train_results
```

```{r include = FALSE}
log_test_results <- log_wf %>% 
  finalize_workflow(log_best) %>%
  last_fit(split = df_split, metrics = model_metrics)
log_results <- log_test_results %>% 
  collect_metrics()
log_results
```

```{r include = FALSE}
log_fit <- log_wf %>%
  finalize_workflow(log_best) %>%
  fit(df_test)
log_fit
```

## Predicting on new data
```{r echo = FALSE}
pred_log <- augment(log_fit, new_data = new_data, type = "prob") %>% 
	select(.pred_class, .pred_Yes, .pred_No)
pred_log
```

# 2.Random Forest model
```{r include = FALSE}
rf_spec <- 
  rand_forest(mtry = tune(),
              min_n = tune(),
              trees = tune()) %>%
  set_engine("ranger") %>% 
  set_mode("classification")
```

```{r include = FALSE}
rf_wflow <-
  workflow() %>%
  add_recipe(clean_rec) %>% 
  add_model(rf_spec)
```

```{r include = FALSE}
rf_grid <- grid_latin_hypercube(
  finalize(mtry(), df_train),
  trees(),
  min_n(),
  size = 15)
```

```{r include = FALSE}
doParallel::registerDoParallel()
set.seed(2022)
rf_res <-
  rf_wflow %>% 
  tune_grid(
    resamples = folds, 
    metrics = model_metrics,
    control = model_control,
    grid = rf_grid)
```

```{r include = FALSE}
rf_best <- rf_res %>% select_best(metric = "accuracy")
autoplot(rf_res)
rf_best
```

```{r include = FALSE}
rf_train_results <- rf_res %>% 
  filter_parameters(parameters = rf_best) %>% 
  collect_metrics()
rf_train_results
```

```{r include = FALSE}
rf_test_res <- rf_wflow %>% 
  finalize_workflow(rf_best) %>%
  last_fit(split = df_split, metrics = model_metrics)
rf_results <- rf_test_res %>% collect_metrics()
rf_results
```

```{r include = FALSE}
rf_fit <- rf_wflow %>%
  finalize_workflow(rf_best) %>%
  fit(df_test)
rf_fit
```

## Predicting on new data
```{r echo = FALSE}
pred_rf <- augment(rf_fit, new_data = new_data, type = "prob") %>% 
	select(.pred_class, .pred_Yes, .pred_No)
pred_rf 
```

# 3.Decision Trees model
```{r include = FALSE}
dt_spec <- 
  decision_tree(
    mode = "classification",
    engine = "rpart",
    cost_complexity = tune(),
    tree_depth = tune(),
    min_n = tune())
```

```{r include = FALSE}
dt_wf <-
  workflow() %>%
  add_recipe(clean_rec) %>% 
  add_model(dt_spec)
```

```{r include = FALSE}
dt_grid <- grid_latin_hypercube(
  cost_complexity(),
  tree_depth(),
  min_n(),
  size = 15)
```

```{r include = FALSE}
doParallel::registerDoParallel()
set.seed(1234)
dt_tune <- dt_wf %>%
  tune_grid(folds,
            metrics = model_metrics,
            control = model_control,
            grid = dt_grid)
```

```{r include = FALSE}
dt_best <- dt_tune %>% select_best(metric = "accuracy")
autoplot(dt_tune)
dt_best
```

```{r include = FALSE}
dt_train_results <- dt_tune %>%
  filter_parameters(parameters = dt_best) %>%
  collect_metrics()
dt_train_results
```

```{r include = FALSE}
dt_test_results <- dt_wf %>% 
  finalize_workflow(dt_best) %>%
  last_fit(split = df_split, metrics = model_metrics)
dt_results <- dt_test_results %>% collect_metrics()
dt_results
```

```{r include = FALSE}
dt_fit <- dt_wf %>%
  finalize_workflow(dt_best) %>%
  fit(df_test)
dt_fit
```

## Predicting on new data
```{r echo = FALSE}
pred_dt <- augment(dt_fit, new_data = new_data, type = "prob") %>% 
	select(.pred_class, .pred_Yes, .pred_No)
pred_dt
```

```{r include = FALSE}
train_results <- bind_rows("Logistic Regression" = log_train_results,
                           "Random Forest" = rf_train_results,
                           "Decision Trees" = dt_train_results,
                           .id = "model")
```

# 4.Comparing results
```{r fig 01}
train_results %>%
  mutate(model = fct_reorder(model, mean)) %>%
  ggplot(aes(.metric, mean, fill = model)) +
  geom_col(position = "dodge") + 
  geom_text(
    aes(x = .metric, y = mean , label = round(mean, 2), group = model),
    position = position_dodge(width = 1), vjust = -0.6, size = 3.5) +
  scale_fill_brewer(palette = "Dark2") +
  labs(y = "Value (mean)", x = "Metric", title =  "Results on training set", fill = "Model:") +
	theme(text = element_text(size = 16))
```

```{r include = FALSE}
test_results <- bind_rows("Logistic Regression" = log_results,
                          "Random Forest" = rf_results,
                          "Decision Trees" = dt_results,
                           .id = "model")
```

```{r fig 02}
test_results %>%
  mutate(model = fct_reorder(model, .estimate)) %>%
  ggplot(aes(.metric, .estimate, fill = model)) +
  geom_col(position = "dodge") + 
  geom_text(
    aes(x = .metric, y = .estimate , label = round(.estimate, 2), group = model),
    position = position_dodge(width = 1), vjust = -0.6, size = 3.5) +
  scale_fill_brewer(palette = "Dark2") +
  labs(y = "Value", x = "Metric", title =  "Results on test set", fill = "Model:") +
	theme(text = element_text(size = 16))
```

# 5.Confusion matrices
```{r fig 03}
collect_predictions(log_test_results) %>%
  conf_mat(creditability, .pred_class) %>%
  pluck(1) %>%
  as_tibble() %>%
  ggplot(aes(Prediction, Truth, alpha = n)) +
  geom_tile(show.legend = FALSE) +
  geom_text(aes(label = n), colour = "white", alpha = 1, size = 8) +
  labs(y = "Actual result", x = "Predicted result", fill = NULL, title = "Confusion Matrix - Logistic Regression") +
	theme(text = element_text(size = 16))
```

```{r fig 04}
collect_predictions(rf_test_res) %>%
  conf_mat(creditability, .pred_class) %>%
  pluck(1) %>%
  as_tibble() %>%
  ggplot(aes(Prediction, Truth, alpha = n)) +
  geom_tile(show.legend = FALSE) +
  geom_text(aes(label = n), colour = "white", alpha = 1, size = 8) +
  labs(y = "Actual result", x = "Predicted result", fill = NULL, title = "Confusion Matrix - Random Forest") +
	theme(text = element_text(size = 16))
```

```{r fig 05}
collect_predictions(dt_test_results) %>%
  conf_mat(creditability, .pred_class) %>%
  pluck(1) %>%
  as_tibble() %>%
  ggplot(aes(Prediction, Truth, alpha = n)) +
  geom_tile(show.legend = FALSE) +
  geom_text(aes(label = n), colour = "white", alpha = 1, size = 8) +
  labs(y = "Actual result", x = "Predicted result", fill = NULL, title = "Confusion Matrix - Decision Trees") +
	theme(text = element_text(size = 16))
```

# 6.ROC curves
```{r fig 06}
log_test_results %>%
  collect_predictions() %>%
  roc_curve(creditability, .pred_No) %>%
  ggplot(aes(x = 1 - specificity, y = sensitivity)) +
  geom_line(size = 0.5, color = "midnightblue") +
  geom_abline(lty = 2, alpha = 0.5, color = "gray50", size = 0.5) + 
  labs(title = "Logistic Regression - ROC curve") +
	theme(text = element_text(size = 16))
```

```{r fig 07}
rf_test_res %>%
  collect_predictions() %>%
  roc_curve(creditability, .pred_No) %>%
  ggplot(aes(x = 1 - specificity, y = sensitivity)) +
  geom_line(size = 0.5, color = "midnightblue") +
  geom_abline(lty = 2, alpha = 0.5, color = "gray50", size = 0.5) + 
  labs(title = "Random Forest - ROC curve") +
	theme(text = element_text(size = 16))
```

```{r fig 08}
dt_test_results %>%
  collect_predictions() %>%
  roc_curve(creditability, .pred_No) %>%
  ggplot(aes(x = 1 - specificity, y = sensitivity)) +
  geom_line(size = 0.5, color = "midnightblue") +
  geom_abline(lty = 2, alpha = 0.5, color = "gray50", size = 0.5) + 
  labs(title = "Decision Trees - ROC curve") +
	theme(text = element_text(size = 16))
```

# 7.Variable Importance
```{r fig 09}
log_test_results %>% 
  pluck(".workflow", 1) %>%
  extract_fit_parsnip() %>% 
  vip(geom = "col", num_features = 10, horiz = TRUE, aesthetics = list(size = 4)) +
  labs(title = "Variable Importance - Logistic Regression") +
	theme(text = element_text(size = 16))
```

```{r fig 10}
dt_test_results %>% 
  pluck(".workflow", 1) %>%
  extract_fit_parsnip() %>% 
  vip(geom = "col", num_features = 10, horiz = TRUE, aesthetics = list(size = 4)) +
  labs(title = "Variable Importance - Decision Trees") +
	theme(text = element_text(size = 16))
```

# 8.Plotting the Decision Tree
```{r fig 11}
dt_test_results %>%
  extract_fit_engine() %>%
  rpart.plot(roundint = FALSE, cex = 0.6)
```